{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "ccb1a2c0_e466d793",
        "filename": "src/plugins/crypto_sw_scheduler/main.c",
        "patchSetId": 1
      },
      "lineNbr": 474,
      "author": {
        "id": 1849
      },
      "writtenOn": "2022-10-17T08:48:07Z",
      "side": 0,
      "message": "The reason we used \"j !\u003d head\" is because eventually head/tail will overflow and falls back to 0. Your modification made the condition check always false when head overflows but tail does not.\n\nThe fix, in my view, is not valid - but the problem you stated is more alarming. Can you provide backtrace when the problem happens instead?",
      "range": {
        "startLine": 474,
        "startChar": 16,
        "endLine": 474,
        "endChar": 20
      },
      "revId": "80c0ae24378f249b3be9a02774d844c13143cd99",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b3e395c5_351cbb76",
        "filename": "src/plugins/crypto_sw_scheduler/main.c",
        "patchSetId": 1
      },
      "lineNbr": 474,
      "author": {
        "id": 1761
      },
      "writtenOn": "2022-10-17T08:57:34Z",
      "side": 0,
      "message": "Sure\n\nOct 14 21:55:50 pb1-2650l2-2204 vpp[675165]: vlib_worker_thread_barrier_sync_int: worker thread deadlock\nOct 14 21:55:50 pb1-2650l2-2204 vnet[675165]: received signal SIGABRT, PC 0x7fd91019ea7c\nOct 14 21:55:50 pb1-2650l2-2204 vnet[675165]: #0  0x00007fd9105707e2 0x7fd9105707e2\nOct 14 21:55:50 pb1-2650l2-2204 vnet[675165]: #1  0x00007fd91014a520 0x7fd91014a520\nOct 14 21:55:50 pb1-2650l2-2204 vnet[675165]: #2  0x00007fd91019ea7c pthread_kill + 0x12c\nOct 14 21:55:50 pb1-2650l2-2204 vnet[675165]: #3  0x00007fd91014a476 raise + 0x16\nOct 14 21:55:50 pb1-2650l2-2204 vnet[675165]: #4  0x00007fd9101307f3 abort + 0xd3\nOct 14 21:55:50 pb1-2650l2-2204 vnet[675165]: #5  0x0000556b3267f8a0 0x556b3267f8a0\nOct 14 21:55:50 pb1-2650l2-2204 vnet[675165]: #6  0x00007fd910553b6d vlib_worker_thread_barrier_sync_int + 0x4ed\nOct 14 21:55:50 pb1-2650l2-2204 vnet[675165]: #7  0x00007fd911facfa1 0x7fd911facfa1\nOct 14 21:55:50 pb1-2650l2-2204 vnet[675165]: #8  0x00007fd911facc53 vl_mem_api_handle_msg_main + 0x63\nOct 14 21:55:50 pb1-2650l2-2204 vnet[675165]: #9  0x00007fd911fb689e 0x7fd911fb689e\nOct 14 21:55:50 pb1-2650l2-2204 vnet[675165]: #10 0x00007fd9105080f7 0x7fd9105080f7\nOct 14 21:55:50 pb1-2650l2-2204 vnet[675165]: #11 0x00007fd9104897d4 0x7fd9104897d4\n\n\ngdb) info threads\n  Id   Target Id                        Frame \n* 1    Thread 0x7f9a4a2a9c40 (LWP 1136) __pthread_kill_implementation (no_tid\u003d0, signo\u003d6, threadid\u003d140300645997632) at ./nptl/pthread_kill.c:44\n  2    Thread 0x7f99ff6a3640 (LWP 1142) vlib_worker_thread_barrier_check () at ../work/vpp/src/vlib/threads.h:388\n  3    Thread 0x7f9a002a6640 (LWP 1139) 0x00007f9a4a3d0fde in epoll_wait (epfd\u003d15, events\u003d0x7f9a002a5d30, maxevents\u003d1, timeout\u003d-1)\n    at ../sysdeps/unix/sysv/linux/epoll_wait.c:30\n  4    Thread 0x7f99ff8a4640 (LWP 1141) 0x00007f9a4a6a5b23 in vlib_worker_thread_barrier_check () at ../work/vpp/src/vlib/threads.h:388\n  5    Thread 0x7f99ffaa5640 (LWP 1140) crypto_sw_scheduler_dequeue (vm\u003dvm@entry\u003d0x7f9a0f55a380, nb_elts_processed\u003dnb_elts_processed@entry\u003d0x7f99ffaa4bec, \n    enqueue_thread_idx\u003denqueue_thread_idx@entry\u003d0x7f99ffaa4bfc) at ../work/vpp/src/plugins/crypto_sw_scheduler/main.c:474\n\t\n(gdb) p current_queue-\u003ehead\n$3 \u003d 80843\n(gdb) p j\n$4 \u003d 511256293\n(gdb) p current_queue-\u003etail\n$2 \u003d 80843\n\n\nMostly, it\u0027s happening when using ipsecmb backend(aes128-gcm16 at current point)",
      "parentUuid": "ccb1a2c0_e466d793",
      "range": {
        "startLine": 474,
        "startChar": 16,
        "endLine": 474,
        "endChar": 20
      },
      "revId": "80c0ae24378f249b3be9a02774d844c13143cd99",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "bc028c40_2afd91da",
        "filename": "src/plugins/crypto_sw_scheduler/main.c",
        "patchSetId": 1
      },
      "lineNbr": 474,
      "author": {
        "id": 1849
      },
      "writtenOn": "2022-10-17T08:59:47Z",
      "side": 0,
      "message": "What is the cli command you used?",
      "parentUuid": "b3e395c5_351cbb76",
      "range": {
        "startLine": 474,
        "startChar": 16,
        "endLine": 474,
        "endChar": 20
      },
      "revId": "80c0ae24378f249b3be9a02774d844c13143cd99",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "242a1968_445cbd0a",
        "filename": "src/plugins/crypto_sw_scheduler/main.c",
        "patchSetId": 1
      },
      "lineNbr": 474,
      "author": {
        "id": 1761
      },
      "writtenOn": "2022-10-17T09:04:28Z",
      "side": 0,
      "message": "All of those configured via API, none of the CLI used. I could try to reproduce using just CLI, but I\u0027ll need some time for that",
      "parentUuid": "bc028c40_2afd91da",
      "range": {
        "startLine": 474,
        "startChar": 16,
        "endLine": 474,
        "endChar": 20
      },
      "revId": "80c0ae24378f249b3be9a02774d844c13143cd99",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "0a773df1_59efcc22",
        "filename": "src/plugins/crypto_sw_scheduler/main.c",
        "patchSetId": 1
      },
      "lineNbr": 474,
      "author": {
        "id": 366
      },
      "writtenOn": "2022-10-18T19:19:17Z",
      "side": 0,
      "message": "The issue seems to be that the load/store of head and tail are being reordered so that head is retrieved before tail is and we end up with head \u003c tail periodically. I added a debug statement \u0026 broke the loop like this:\n\ndiff --git a/src/plugins/crypto_sw_scheduler/main.c b/src/plugins/crypto_sw_scheduler/main.c\nindex 47fa37d72..4434fe0f3 100644\n--- a/src/plugins/crypto_sw_scheduler/main.c\n+++ b/src/plugins/crypto_sw_scheduler/main.c\n@@ -474,6 +474,11 @@ crypto_sw_scheduler_process_aead (vlib_main_t *vm,\n              for (j \u003d tail; j !\u003d head; j++)\n                {\n \n+                 if (j \u003e head)\n+                   {\n+                     clib_warning (\"thread %u i %u tail %u head %u\", vm-\u003ethread_index, i, tail, head);\n+                     break;\n+                   }\n                  f \u003d current_queue-\u003ejobs[j \u0026 CRYPTO_SW_SCHEDULER_QUEUE_MASK];\n \n                  if (!f)\n\nThe messages logged when traffic was sent through the tunnels look like this:\n\nOct 18 14:35:51 NG-6100-DUT-1 vnet[5440]: crypto_sw_scheduler_dequeue:479: thread 2 i 1 tail 82787 head 82785    \nOct 18 14:35:57 NG-6100-DUT-1 vnet[5440]: crypto_sw_scheduler_dequeue:479: thread 3 i 1 tail 307370 head 307366  \nOct 18 14:36:30 NG-6100-DUT-1 vnet[5440]: crypto_sw_scheduler_dequeue:479: thread 3 i 2 tail 592032 head 591921  \nOct 18 14:37:53 NG-6100-DUT-1 vnet[5440]: crypto_sw_scheduler_dequeue:479: thread 2 i 1 tail 1685293 head 1685288\n\nAs you can see, tail \u003e head and it is not due to head overflowing.\n\nI tried to put a call to CLIB_MEMORY_STORE_BARRIER() in between the assignment of tail and the assignment of head (in between lines 471 and 472 on current master branch). That seemed to prevent the deadlock/crash, but I have no idea what the performance implications are and I\u0027m not sure if that\u0027s the correct approach or if a different type of barrier should be used.\n\nIf that doesn\u0027t seem like the right thing to do, we could instead mask the value of head and tail and mask j after it is incremented. In that case, if tail \u003e head, j would end up being equal to head within 64 iterations.",
      "parentUuid": "242a1968_445cbd0a",
      "range": {
        "startLine": 474,
        "startChar": 16,
        "endLine": 474,
        "endChar": 20
      },
      "revId": "80c0ae24378f249b3be9a02774d844c13143cd99",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "b287b39f_782b8ed1",
        "filename": "src/plugins/crypto_sw_scheduler/main.c",
        "patchSetId": 1
      },
      "lineNbr": 474,
      "author": {
        "id": 1849
      },
      "writtenOn": "2022-10-19T14:02:28Z",
      "side": 0,
      "message": "Hi Matthew, thanks for the research and great results presented. If cores are not isolated yes this may cause the reordered load/store.\n\nForcing a sync every time reading head/tail may have some perf impact - I suspect when the number of threads increases the perf impact shall grow accordingly.\n\nFrom the log when tail \u003e head happens - I feel it is not frequent (per 30 seconds)? We may add a check @line 473 to check if tail \u003e head and it is not caused by overflown counters, if yes the thread abandon scanning the queue for not processed frames this time and try again in next loop - this may resolve the problem and we will have minimum performance impact.",
      "parentUuid": "0a773df1_59efcc22",
      "range": {
        "startLine": 474,
        "startChar": 16,
        "endLine": 474,
        "endChar": 20
      },
      "revId": "80c0ae24378f249b3be9a02774d844c13143cd99",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    }
  ]
}