{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "a1eed959_a089c87d",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 5
      },
      "lineNbr": 0,
      "author": {
        "id": 1849
      },
      "writtenOn": "2022-11-29T09:47:37Z",
      "side": 1,
      "message": "We had this problem before but it is long gone - as of now both async crypto engines work fine even with really high traffic rate. Can you explain when and how you notice the crash?\n\nAlso as the patch itself I don\u0027t think simply replacing the frame elt pointer to index will resolve the issue.",
      "revId": "d7b3dba333e482a74698f1171e41ab1e53dbbc3e",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "984ef259_ea2097a5",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 5
      },
      "lineNbr": 0,
      "author": {
        "id": 1737
      },
      "writtenOn": "2022-11-29T10:17:52Z",
      "side": 1,
      "message": "You are wrong. We have also tested the ipsec async routine, we found if the \n vnet buffer\u0027s num extend to 165536, ipsec will very likely crash due to frame invalid when using if pool expended.\nWe register our self sm3/sm4 async enqueue/dequeue handle and use the fpga which made by my company to speed up sm3/sm4 throughput and the total throughput can be 20gbps.\nWe have tested the async crypto engine and got the crash.\nWith this patch, the issue is fixed\n\nEvent without the crash, if reviewing the async code, you will find there are possibility to crash.\n\nov 22 14:40:09 localhost vpp[891]: received signal SIGSEGV, PC 0x7fc9a0e1a817, faulting address 0x7ff930954134\nNov 22 14:40:09 localhost vpp[891]: #0  0x00007fc9a1412484 unix_signal_handler + 0x124\nNov 22 14:40:09 localhost vpp[891]: #1  0x00007fc99fc515e0 0x7fc99fc515e0\nNov 22 14:40:09 localhost vpp[891]: #2  0x00007fc9a0e1a817 ipsec4_input_node_fn_skx + 0x477\nNov 22 14:40:09 localhost vpp[891]: #3  0x00007fc9a13bfcf5 dispatch_pending_node + 0x135\nNov 22 14:40:09 localhost vpp[891]: #4  0x00007fc9a13c17df vlib_worker_loop + 0x5ef\nNov 22 14:40:09 localhost vpp[891]: #5  0x00007fc9a13f4d5d vlib_worker_thread_fn + 0xcd\nNov 22 14:40:09 localhost clixon_backend[31649]: [utils.c:1422 exec_vpp_command] exec: security policy create name ssfff action drop priority 1",
      "parentUuid": "a1eed959_a089c87d",
      "revId": "d7b3dba333e482a74698f1171e41ab1e53dbbc3e",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "530c65a1_ab9ddaae",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 5
      },
      "lineNbr": 0,
      "author": {
        "id": 1737
      },
      "writtenOn": "2022-11-29T10:45:04Z",
      "side": 1,
      "message": "You can test ipsec with ASAN enabled in debug mode or just check the frame whether in pool in release mode, you will very likely get the memory issue.",
      "parentUuid": "984ef259_ea2097a5",
      "revId": "d7b3dba333e482a74698f1171e41ab1e53dbbc3e",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "10321323_26c89e76",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 5
      },
      "lineNbr": 0,
      "author": {
        "id": 1849
      },
      "writtenOn": "2022-11-29T10:46:53Z",
      "side": 1,
      "message": "I doubt the patch is a fix as you only add two steps - getting the offset of pool elt from pool head, and using the offset the get the pool elt. The way pool is allocated and freed, and the elt get/put ops remain the same.\n\nAlso\n\n- SM3/4 are not officially supported in vpp crypto infra (yet), so as your FPGA based engine. Your statement might be true that the patch addressed your problem, but this also may indicate there are some problems in your implementation, yet I cannot judge as I have no visibility to it.\n\n- vlib buffer pool of 160k is a huge number, that means your vlib buffer alone takes about 380MB per thread (with default 2KB data room). I can only imagine your FPGA engine latency is extremely high, otherwise I don\u0027t understand why you need such a huge pool size (as your thread will return the buffers back to pool when TX is done). I have never seen such an extreme requirement before - in normal cases 512 is enough.\n\nIf 160K is really necessary, what is the performance you can get with 80K/40k/20k.../512 buffer size?",
      "parentUuid": "984ef259_ea2097a5",
      "revId": "d7b3dba333e482a74698f1171e41ab1e53dbbc3e",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "fcb5c9b0_0e68800f",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 5
      },
      "lineNbr": 0,
      "author": {
        "id": 1849
      },
      "writtenOn": "2022-11-29T10:51:38Z",
      "side": 1,
      "message": "Sorry I made a mistake, it is 380MB per numa, not per thread.",
      "parentUuid": "10321323_26c89e76",
      "revId": "d7b3dba333e482a74698f1171e41ab1e53dbbc3e",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "f3f73a0d_9cbdfe39",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 5
      },
      "lineNbr": 0,
      "author": {
        "id": 1849
      },
      "writtenOn": "2022-11-29T10:52:47Z",
      "side": 1,
      "message": "What is your rx/tx descriptor settings?",
      "parentUuid": "fcb5c9b0_0e68800f",
      "revId": "d7b3dba333e482a74698f1171e41ab1e53dbbc3e",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "fddd2ac5_71137fdf",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 5
      },
      "lineNbr": 0,
      "author": {
        "id": 1737
      },
      "writtenOn": "2022-11-29T11:08:15Z",
      "side": 1,
      "message": "The async_frames[async_op] saved the frame* addr alloc by vnet_crypto_async_get_frame, there is a time when async_op not 0 and cf-\u003eframe_pool \n size will be expended by pool_get_aligned, the frames address saved in async_frames[async_op] will invalid due to cf-\u003eframe_pool address changed.\nIn my engine, I save the frame index calculated by (frmae - ct-\u003eframe_pool) when enqueuing to my engine for dequeue using, i find the index may be very large and index is out of cf-\u003eframe_pool.",
      "parentUuid": "f3f73a0d_9cbdfe39",
      "revId": "d7b3dba333e482a74698f1171e41ab1e53dbbc3e",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    }
  ]
}