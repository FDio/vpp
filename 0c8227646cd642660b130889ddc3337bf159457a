{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "689c6642_ea8aa82f",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 680
      },
      "writtenOn": "2025-09-05T18:25:17Z",
      "side": 1,
      "message": "To minimize the impact of the lock, I think we should use clib_spinlock_trylock. If the lock is held by the other thread, the other thread will do the kick. This thread does not have to wait and just continue.",
      "revId": "0c8227646cd642660b130889ddc3337bf159457a",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "106e9b56_9027f860",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 680
      },
      "writtenOn": "2025-09-05T18:28:56Z",
      "side": 1,
      "message": "I mean clib_spinlock_trylock_if_init",
      "parentUuid": "689c6642_ea8aa82f",
      "revId": "0c8227646cd642660b130889ddc3337bf159457a",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "7b2846a5_e3796bb8",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1388
      },
      "writtenOn": "2025-09-08T08:28:56Z",
      "side": 1,
      "message": "I guess try_lock() would work well if the interrupt triggering logic in different threads was the same. However, the input logic in vhost_user_input_do_interrupt() is responsible for triggering an interrupt by time whereas the output logic in the device TX function triggers an interrupt when a certain number of frames piled up. Trying to lock, failing, and then skipping one of those blocks may cause undesired effects. For example, the current number of frames is low but it\u0027s time to trigger an interrupt by time. And vice versa, the required number of frames has piled up but it\u0027s early to trigger an interrupt by time. If the appropriate block is missed, probably many times in a row, there will be zero interrupts for some time. Moreover, the device TX function also increments the number of frames. If skip that block often, there will not be data to decide to trigger an interrupt.",
      "parentUuid": "106e9b56_9027f860",
      "revId": "0c8227646cd642660b130889ddc3337bf159457a",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "d88c2c48_2dcf5183",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 680
      },
      "writtenOn": "2025-09-09T19:12:14Z",
      "side": 1,
      "message": "Can we narrow down the try_lock() in the function to skip the write system call only?\nvhost_user_kick (vlib_main_t * vm, vhost_user_vring_t * vq)\n  rv \u003d write (fd, \u0026x, sizeof (x)); \u003c\u003c\u003c\u003c\nWe can use the atomic operations to avoid simultaneous access to the other controlling variables if need be.\n\nI am worried about the performance impact of the patch that disallows parallel processing for both input and output threads for a period of time, ie, when the input thread gets the lock, it issues the write system call which may take a bit of time. During that period, the output thread is waiting for the spinlock.\n\nDo you have any performance number to compare between the baseline and with the patch?",
      "parentUuid": "7b2846a5_e3796bb8",
      "revId": "0c8227646cd642660b130889ddc3337bf159457a",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "0b850954_470b5cfa",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 1
      },
      "lineNbr": 0,
      "author": {
        "id": 1388
      },
      "writtenOn": "2025-09-12T12:47:08Z",
      "side": 1,
      "message": "\u003e Can we narrow down the try_lock() in the function to skip the write system call only?\n\nWe don\u0027t have to run the system call under a lock. It would be enough to protect the controlling variables only. However, once the system call is made in vhost_user_kick(), the controlling variables are modified too. That\u0027s the reason why in this patch the system call and the whole vhost_user_kick() runs under a lock.\n\n\u003e I am worried about the performance impact of the patch that disallows parallel processing for both input and output threads for a period of time, [...]\n\nI\u0027m sorry I didn\u0027t mention this in the commit description, but the goal of the patch is not just to fix the race condition. The end goal is to improve performance of a thread avoiding unnecessary interrupts that result in system calls. Because the less system calls a thread makes, the less time it\u0027s blocked and can do other work.\n\nHowever, your concern is valid. While potentially improving the performance of one thread we may reduce the performance of another.\n\n\u003e Do you have any performance number to compare between the baseline and with the patch?\n\nIt depends on what numbers you have in mind. Throughput between two VMs remains on the same level. The number of interrupts for rxvq halves.\n\nVM1 -\u003e VM2\nVirtualEthernet0/0/0 (Thread 1) -\u003e VirtualEthernet0/0/1 (Thread 2)\ncoalesce frames 63 time 1e-4\n60s iperf3 TCP benchmark\nMTU 1500\n\nThroughput is around 10Gbps with or without this patch. The number of rxvq interrupts vary from run to run, because the traffic pattern may change as well as the number of race conditions. But with this patch, the number of rxvq interrupts is consistently around two times lower. For example, here are the numbers of rxvq interrupts for two sequential tests: before 435832, after 206110.\n\n-----\n\nI will rework the patch to avoid making the system call under a lock. This should potentially improve the performance of a thread that makes the system call while not blocking the other thread.",
      "parentUuid": "d88c2c48_2dcf5183",
      "revId": "0c8227646cd642660b130889ddc3337bf159457a",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "13186201_acfa4f04",
        "filename": "src/plugins/vhost/vhost_user.c",
        "patchSetId": 1
      },
      "lineNbr": 307,
      "author": {
        "id": 680
      },
      "writtenOn": "2025-09-04T17:09:58Z",
      "side": 1,
      "message": "Should we only call clib_spinlock_init for the rxq? That way we can remove the test is_rxq in the loop in the function vhost_user_send_interrupt_process.",
      "revId": "0c8227646cd642660b130889ddc3337bf159457a",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "f99d79de_9a49e4ed",
        "filename": "src/plugins/vhost/vhost_user.c",
        "patchSetId": 1
      },
      "lineNbr": 307,
      "author": {
        "id": 1388
      },
      "writtenOn": "2025-09-05T12:28:07Z",
      "side": 1,
      "message": "int_lock mainly follows vring_lock, which is also only used for rxvq but initialized for both directions. I think we should probably leave this the way it is as an acceptable way.",
      "parentUuid": "13186201_acfa4f04",
      "revId": "0c8227646cd642660b130889ddc3337bf159457a",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    }
  ]
}