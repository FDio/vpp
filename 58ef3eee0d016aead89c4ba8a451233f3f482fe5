{
  "comments": [
    {
      "unresolved": false,
      "key": {
        "uuid": "edd0ce81_54a81dac",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 3
      },
      "lineNbr": 0,
      "author": {
        "id": 2220
      },
      "writtenOn": "2024-05-08T12:36:48Z",
      "side": 1,
      "message": "out of curiosity, can\u0027t we flush handover queue before going into worker barrier?",
      "revId": "58ef3eee0d016aead89c4ba8a451233f3f482fe5",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "90359a43_96bf08b8",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 3
      },
      "lineNbr": 0,
      "author": {
        "id": 1849
      },
      "writtenOn": "2024-05-16T19:25:58Z",
      "side": 1,
      "message": "I don\u0027t think we have a way to flush the handoff queue neatly at the moment, as this means implementing a way for main core to inform all worker cores to block handing off packets and drain the hand-off queues ASAP (and there is no way for esp nodes to know if the queue is drained!). Also I don\u0027t think this is necessary.",
      "parentUuid": "edd0ce81_54a81dac",
      "revId": "58ef3eee0d016aead89c4ba8a451233f3f482fe5",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "7b11cee8_280eb66c",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 3
      },
      "lineNbr": 0,
      "author": {
        "id": 2220
      },
      "writtenOn": "2024-05-21T07:53:05Z",
      "side": 1,
      "message": "while I understand that this is out of scope for this particular patch - I don\u0027t completely agree with you. indeed now we don\u0027t have a nice way to overcome it.\n\nanyway, this approach only \"fixes\" esp encrypt code while we have many places with handoff packets. what about esp_decrypt, ah_encrypt, ah_decrypt?\n\nin general, the similar approach should(?) be applied to all the places where we have handoff + management of dp accessed objects via api. I just thought what if there was a way to get rid off this vector of error proneness..",
      "parentUuid": "90359a43_96bf08b8",
      "revId": "58ef3eee0d016aead89c4ba8a451233f3f482fe5",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "d25a38c1_1355194f",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 3
      },
      "lineNbr": 0,
      "author": {
        "id": 1849
      },
      "writtenOn": "2024-05-23T20:33:48Z",
      "side": 1,
      "message": "I believe this is all down to the synchronization between cp and multi-core dp - and this is a common problem not limited to vpp. Handed-off packets are not second class citizens in vpp land actually, each core\u0027s main loop actually deals with the hand-off queues first. Hand-off also helps improving performance in many places such as NAT and crypto scheduler.\n\nThe only question is when cp issues a config change affecting packets waiting in the hand-off queues, should we let these packets processed before the config is effective, or should we apply the packets processed by the new config?\n\nAs I mentioned, choosing the former way is not impossible but I have to believe it is costly. There are at least 3 messages between main core and every worker, and the workers finished draining early have to wait for the slower cores before continuing (only all queues drained the main core can apply the new config). If you have a better way please share, much appreciated!",
      "parentUuid": "7b11cee8_280eb66c",
      "revId": "58ef3eee0d016aead89c4ba8a451233f3f482fe5",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "1969d205_257649e0",
        "filename": "/PATCHSET_LEVEL",
        "patchSetId": 3
      },
      "lineNbr": 0,
      "author": {
        "id": 2220
      },
      "writtenOn": "2024-06-04T09:29:08Z",
      "side": 1,
      "message": "Hello Fan,\nI tried to implement and now I see what you meant. You\u0027re 100% right, there\u0027s no easy (and beautiful) way to fix this. \n\nI think that approach in this particular fix should be applied also on all nodes in ipsec code.",
      "parentUuid": "d25a38c1_1355194f",
      "revId": "58ef3eee0d016aead89c4ba8a451233f3f482fe5",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    }
  ]
}