{
  "comments": [
    {
      "unresolved": true,
      "key": {
        "uuid": "fd99ce4f_07cf0911",
        "filename": "src/vnet/tcp/tcp_input.c",
        "patchSetId": 2
      },
      "lineNbr": 2609,
      "author": {
        "id": 193
      },
      "writtenOn": "2025-07-23T07:38:10Z",
      "side": 1,
      "message": "Because the connection is not cleaned up here, I\u0027d expect the lookup on line 2626 will hit exactly the same connection. During test do you happen to see error create exists incrementing?",
      "revId": "11e946cee85ecd89051957ea1fbc29139cd09810",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "a618e220_eed61eaa",
        "filename": "src/vnet/tcp/tcp_input.c",
        "patchSetId": 2
      },
      "lineNbr": 2609,
      "author": {
        "id": 3242
      },
      "writtenOn": "2025-07-24T00:48:07Z",
      "side": 1,
      "message": "Thanks for the comment.\n\nI tested this patch under stress conditions for CPS testing, but I didn’t observe any incrementing behavior. From what I can tell, the lookup is performed using a lookup table, and the connection in the case has already been removed from the table and is currently in pending_cleanups.\n\nPlease feel free to share any additional thoughts if I’ve missed something.",
      "parentUuid": "fd99ce4f_07cf0911",
      "revId": "11e946cee85ecd89051957ea1fbc29139cd09810",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "9f5ea887_3b7715ca",
        "filename": "src/vnet/tcp/tcp_input.c",
        "patchSetId": 2
      },
      "lineNbr": 2609,
      "author": {
        "id": 3242
      },
      "writtenOn": "2025-07-24T13:51:13Z",
      "side": 1,
      "message": "Done",
      "parentUuid": "a618e220_eed61eaa",
      "revId": "11e946cee85ecd89051957ea1fbc29139cd09810",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "484238f6_86863c86",
        "filename": "src/vnet/tcp/tcp_input.c",
        "patchSetId": 2
      },
      "lineNbr": 2609,
      "author": {
        "id": 193
      },
      "writtenOn": "2025-07-25T05:37:02Z",
      "side": 1,
      "message": "The lookup done by tcp_lookup_connection uses the same lookup table that is used in tcp-input before sending to tcp-listen node. Without tcp_connection_cleanup_and_notify, which is called by the cleanup handler, there\u0027s typically no call to session_lookup_del_session, so the connection is not going to be removed from the session table. \n\nWondering now if your test is only exercising resets in syn-sent state. Those will cleanup everything, including lookup table, on first reset. But I don\u0027t think it\u0027s common that we\u0027ll be getting a syn that fast after a reset. \n\nIs there any way to reproduce your test and to test this maybe in our ci?",
      "parentUuid": "9f5ea887_3b7715ca",
      "revId": "11e946cee85ecd89051957ea1fbc29139cd09810",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "ed9715d3_3e2ba0ef",
        "filename": "src/vnet/tcp/tcp_input.c",
        "patchSetId": 2
      },
      "lineNbr": 2609,
      "author": {
        "id": 3242
      },
      "writtenOn": "2025-07-28T07:03:51Z",
      "side": 1,
      "message": "Unfortunately, I\u0027m not yet familiar with your CI and test framework, but I hope to get up to speed soon.\n\nLet me explain the situation I\u0027m facing. I\u0027m trying to measure the CPS capability of the proxy HS app in VPP. For this, I chose Nginx as the HTTP server and K6 from Grafana as the client. VPP delivers traffic between them via the \"proxy app,\" which I slightly modified to make the proxy transparent not only for IP but also for the port. So, for example, if K6 initiates a request from source port 1024, the (AO) VPP proxy makes the request to the server using the same port.\n\nWith this setup, when I run K6 in CPS test mode using HTTP/1.1 with a 1-byte response body and 10,000 or more virtual users (VUs), I observe a significant number of RST packets. That’s where I encountered the issue.\n\nI agree with your point that this behavior is uncommon, but I believe it’s not trivial either. FWIW, I saw the CPS result hits twice than before after fixing along with some other trivial patches. It seems to stem from how K6 handles CPS testing — possibly ignoring the TIME_WAIT state and sending the next SYN immediately after the TCP FIN sequence.\n\nInside VPP, AFAIK, sessions in `TCP_STATE_CLOSED` are placed in `pending_cleanups`, as handled by `tcp_timer_waitclose_handler()`, and later cleared by `tcp_handle_cleanups()`, which is invoked by `tcp_update_time()`. In my initial patch, I used `tcp_connection_cleanup_and_notify()` for those connections, but then `tcp_handle_cleanups()` cleaned up newly connected TCP connections with the same `connection_index` occasionally. So, I let it be kept in `pending_cleanups` as it\u0027ll be handled by `tcp_update_time()` anyway and there\u0027s no way to picking it out from `pending_cleanups` efficiently.",
      "parentUuid": "484238f6_86863c86",
      "revId": "11e946cee85ecd89051957ea1fbc29139cd09810",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": true,
      "key": {
        "uuid": "5a4a8553_fcb1b4fc",
        "filename": "src/vnet/tcp/tcp_input.c",
        "patchSetId": 2
      },
      "lineNbr": 2609,
      "author": {
        "id": 193
      },
      "writtenOn": "2025-07-30T08:12:15Z",
      "side": 1,
      "message": "Quick reply, we have some wrk tests with both builtin proxy and nginx under extras/hs-test. It would be really good if we could expand those to include k6. The changes to the proxy, if not too intrusive we could maybe add as options. \n\nRegarding the cleanup, yes, those were the things I was worried about as well. We can\u0027t force the cleanup because of `pending_cleanups`. At the same time, reusing the connection will most probably lead to surprises, i.e., in a first instance the request will be accepted, if it comes within the 0.1ms of cleanup timeout, but then the connection will then be cleaned up forcefully. There\u0027s also a chance of an attempt at a double free if the connection is closed again by the apps/proxy.",
      "parentUuid": "ed9715d3_3e2ba0ef",
      "revId": "11e946cee85ecd89051957ea1fbc29139cd09810",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    },
    {
      "unresolved": false,
      "key": {
        "uuid": "36067cab_360d2f8e",
        "filename": "src/vnet/tcp/tcp_input.c",
        "patchSetId": 2
      },
      "lineNbr": 2609,
      "author": {
        "id": 3242
      },
      "writtenOn": "2025-08-04T05:34:20Z",
      "side": 1,
      "message": "I tested again to verify whether the closed connection is properly reused, but it wasn\u0027t. So, I updated the code to use `tcp_connection_cleanup_and_notify()`, as you originally suggested.\nStill, pending_cleanups needs to be handled to prevent double-free issues. To address this, I added `tcp_cancel_cleanup()`, which ignores existing cleanup requests by setting the connection_index field to ~0.\nThere might be some efficiency concerns when looking up connections in `pending_cleanups`. However, since this case doesn\u0027t occur frequently, I\u0027m not sure whether using hashing or sorting would be a better approach.\"",
      "parentUuid": "5a4a8553_fcb1b4fc",
      "revId": "11e946cee85ecd89051957ea1fbc29139cd09810",
      "serverId": "6d2eb258-4fe2-443e-8a38-ca81da23d4c2"
    }
  ]
}